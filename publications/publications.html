<!DOCTYPE html> 
<html lang="en">
<head>
    <title>Yorgos Pantis</title>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="generator" content="Hugo 0.113.0">
    <meta name="description" content="Publications by Yorgos Pantis in the field of Machine Learning and Natural Language Processing.">

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Alegreya:wght@500&family=Pacifico&family=Lora:wght@400&family=Indie+Flower&family=Poppins:wght@400;700&family=Bodoni+Moda:wght@400;700&display=swap" rel="stylesheet">

    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.1/dist/css/bootstrap.min.css" rel="stylesheet">
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet">
    <link href="../styles/custom.css" rel="stylesheet">

    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.1/dist/js/bootstrap.bundle.min.js" crossorigin="anonymous"></script>
    <script src="../scripts.js"></script>

    <!-- Style for publication labels -->
    <style>
      .pub-label {
        font-weight: bold;
        font-size: 1rem;
        color: #007bff;
        margin-right: 0.6rem;
        display: inline-block;
        width: 2.5rem;
        font-style: italic;
      }

      .pub-content {
        display: inline-block;
        max-width: calc(100% - 3rem);
        vertical-align: top;
        margin-bottom: 1.5rem;
      }
    </style>
</head>

<body>
    <div id="outer">
        <header>
            <div id="navbar">
                <nav class="navbar navbar-expand-lg navbar-light border-bottom fixed-top col-20 mx-auto">
                    <div class="container-fluid">
                        <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarTogglerDemo03" aria-controls="navbarTogglerDemo03" aria-expanded="false" aria-label="Toggle navigation">
                            <span class="navbar-toggler-icon"></span>
                        </button>
                        <a class="navbar-brand me-auto order-0 fancy-button" href="../index.html" id="name-item">
                            <span id="name-text">Yorgos Pantis</span>
                        </a>
                        <div class="collapse navbar-collapse" id="navbarTogglerDemo03">
                            <ul class="navbar-nav ms-auto mt-2 mt-lg-0 order-1">
                                <li class="nav-item"><a class="nav-link fancy-button" href="../index.html#home">Home</a></li>
                                <li class="nav-item"><a class="nav-link fancy-button" href="../education/education.html">Education</a></li>
                                <li class="nav-item"><a class="nav-link fancy-button" href="../experience/experience.html">Experience</a></li>
                                <li class="nav-item"><a class="nav-link fancy-button" href="publications.html">Publications</a></li>
                                <li class="nav-item"><a class="nav-link fancy-button" href="../honors_awards/honors_awards.html">Honors & Awards</a></li>
                                <li class="nav-item"><a class="nav-link fancy-button" href="../talks_service/talks_service.html">Talks & Service</a></li>
                                <li class="nav-item"><a class="nav-link fancy-button" href="../contact/contact.html">Contact</a></li>
                                <li class="nav-item"><a class="nav-link fancy-button" href="../files/cv_yorgospantis.pdf" target="_blank">CV</a></li>
                            </ul>
                        </div>
                    </div>
                </nav>
            </div>
        </header>

        <main class="card col-20 mx-auto" id="main">
            <div class="row pt-1 m-1">
                <!-- Publications Section Start -->
                <section id="publications">
                    <div class="row content-header">
                        <div class="col pr-0">
                            <p style="margin-bottom: 0;">Publications</p>
				<div style="font-size: 0.9rem; font-style: italic; color: #555; margin-top: 0.2rem;">
    					C: Conference &nbsp;&nbsp; J: Journal &nbsp;&nbsp; †: equal contribution.
				</div>
				<div style="height: 1rem;"></div> <!-- empty line -->
                        </div>
                    </div>

<!-- Conference Paper C3 -->
<div class="publication">
  <span class="pub-label">C<sub>3</sub></span>
  <div class="pub-content">
    <div class="light-bold">A Derandomization Framework for Structure Discovery: Applications in Neural Networks and Beyond.</div>
    <div>
      Nikos Tsikouras, Yorgos Pantis, Ioannis Mitliagkas, Christos Tzamos<br>
      <i>[Under Review]</i>.
    </div>
    <div class="link-button" onclick="toggleAbstract('abstract-c3')">Abstract</div>
    <div class="link-button disabled" style="opacity:0.6; cursor:default;">Paper (coming soon)</div>
    <div class="link-button" onclick="window.open('https://github.com/yorgospantis/StructureDiscovery', '_blank')">Code</div>
    <div class="link-button" onclick="toggleBibtex('bibtex-c3')">BibTeX</div>

    <div id="abstract-c3" class="abstract" style="display:none; border-left-color:#007bff;">
      Understanding the dynamics of feature learning in neural networks (NNs) remains a significant challenge. The work of Mousavi et al. analyzes a multiple index teacher–student setting and shows that a two-layer student attains a low-rank structure in its first-layer weights when trained with stochastic gradient descent (SGD) and a strong regularizer. This structural property is known to reduce sample complexity of generalization. Indeed, in a second step, the same authors establish algorithm-specific learning guarantees under additional assumptions. In this paper, we focus exclusively on the structure discovery aspect and study it under weaker assumptions, more specifically: we allow (a) NNs of arbitrary size and depth, (b) with all parameters trainable, (c) under any smooth loss function, (d) tiny regularization, and (e) trained by any method that attains a second-order stationary point (SOSP), e.g., perturbed gradient descent (PGD). At the core of our approach is a key <i>derandomization</i> lemma, which states that optimizing the function E<sub>x</sub>[g<sub>θ</sub>(W x + b)] converges to a point where W = 0, under mild conditions. The fundamental nature of this lemma directly explains structure discovery and has immediate applications in other domains including an end-to-end approximation for MAXCUT, and computing Johnson–Lindenstrauss embeddings.
    </div>

    <div id="bibtex-c3" class="bibtex" style="display:none; border-left-color:#007bff;">
      @inproceedings{tsikouras2024derandomization,<br>
      title={A Derandomization Framework for Structure Discovery: Applications in Neural Networks and Beyond},<br>
      author={Tsikouras, Nikos and Pantis, Yorgos and Mitliagkas, Ioannis and Tzamos, Christos},<br>
      booktitle={arXiv preprint},<br>
      year={2025}<br>
      }
    </div>
  </div>
</div>

                    <!-- Conference Paper C2† -->
                    <div class="publication">
			<span class="pub-label">C₂†</span>
                        <div class="pub-content">
                            <div class="light-bold">Teaching Transformers to Solve Combinatorial Problems through Efficient Trial & Error.</div>
                            <div>Panagiotis Giannoulis, Yorgos Pantis, Christos Tzamos. <br>
                                <i>Proceedings of the 39th Annual Conference on Neural Information Processing Systems (NeurIPS 2025)</i>.
                            </div>
                            <div class="link-button" onclick="toggleAbstract('abstract-neurips')">Abstract</div>
                            <div class="link-button" onclick="window.open('https://arxiv.org/abs/2509.22023', '_blank')">Paper</div>
                            <div class="link-button" onclick="toggleBibtex('bibtex-neurips')">BibTeX</div>

                            <div id="abstract-neurips" class="abstract" style="display:none; border-left-color:#007bff;">
                                Despite their proficiency in various language tasks, Large Language Models (LLMs) struggle with combinatorial problems like Satisfiability, Traveling Salesman Problem, or even basic arithmetic. We address this gap through a novel approach for solving problems in the class NP. We focus on the paradigmatic task of Sudoku and achieve state-of-the-art accuracy (99%) compared to prior neuro-symbolic approaches. Unlike prior work that used custom architectures, our method employs a vanilla decoder-only Transformer (GPT-2) without external tools or function calling. Our method integrates imitation learning of simple Sudoku rules with an explicit Depth-First Search (DFS) exploration strategy involving informed guessing and backtracking. Moving beyond imitation learning, we seek to minimize the number of guesses until reaching a solution. We provide a rigorous analysis of this setup formalizing its connection to a contextual variant of Min-Sum Set Cover, a well-studied problem in algorithms and stochastic optimization.
                            </div>
                            <div id="bibtex-neurips" class="bibtex" style="display:none; border-left-color:#007bff;">
                                @inproceedings{giannoulis2025teaching,<br>
                                title={Teaching Transformers to Solve Combinatorial Problems through Efficient Trial & Error},<br>
                                author={Giannoulis, Panagiotis and Pantis, Yorgos and Tzamos, Christos},<br>
                                booktitle={Proceedings of the Thirty-ninth Annual Conference on Neural Information Processing Systems},<br>
                                year={2025}<br>
                                }
                            </div>
                        </div>
                    </div>

                    <!-- Journal Paper J1 -->
                    <div class="publication">
                        <span class="pub-label">J₁</span>
                        <div class="pub-content">
                            <div class="light-bold">The Anatomy of Deception: Measuring Technical and Human Factors of a Large-scale Phishing Campaign.</div>
                            <div>Anargyros Chrysanthou, Yorgos Pantis, Constantinos Patsakis. <br>
                                <i>Journal Computers & Security</i>, 2024.
                            </div>
                            <div class="link-button" onclick="toggleAbstract('abstract-phishing')">Abstract</div>
                            <div class="link-button" onclick="window.open('https://www.sciencedirect.com/science/article/abs/pii/S0167404824000816', '_blank')">Paper</div>
                            <div class="link-button" onclick="toggleBibtex('bibtex-phishing')">BibTeX</div>

                            <div id="abstract-phishing" class="abstract" style="display:none; border-left-color:#007bff;">
                                In an era dominated by digital interactions, phishing campaigns have evolved to exploit not just technological vulnerabilities but also human traits. This study takes an unprecedented deep dive into large-scale phishing campaigns aimed at Meta's users, offering a dual perspective on the technical mechanics and human elements involved. Analysing data from over 25,000 victims worldwide, we highlight the nuances of these campaigns, from the intricate techniques deployed by the attackers to the sentiments and behaviours of those targeted. Unlike prior research conducted in controlled environments, this investigation capitalises on the vast, diverse, and genuine data extracted directly from active phishing campaigns, allowing for a more holistic understanding of the drivers, facilitators, and human factors. Through applying advanced computational techniques, including natural language processing and machine learning, this work unveils critical insights into the psyche of victims and the evolving tactics of modern phishers. Our analysis illustrates very poor password selection choices from the victims, with 30.27% of them picking low-complexity passwords and 58.23% reusing leaked passwords. Additionally, more than 10% exhibit strong persistence in re-victimisation by posting again to the phishing platforms of the same phishers. Finally, we reveal many correlations regarding demographics and the time periods when victims are more vulnerable during the day, as well as analyse the sentiment, emotion, and tone of text responses that they submitted, illustrating how convinced they were of the scam.
                            </div>
                            <div id="bibtex-phishing" class="bibtex" style="display:none; border-left-color:#007bff;">
                                @article{chrysanthou2024anatomy,<br>
                                title={The anatomy of deception: Measuring technical and human factors of a large-scale phishing campaign},<br>
                                author={Chrysanthou, Anargyros and Pantis, Yorgos and Patsakis, Constantinos},<br>
                                journal={Computers & Security},<br>
                                volume={140},<br>
                                pages={103780},<br>
                                year={2024},<br>
                                publisher={Elsevier}<br>
                                }
                            </div>
                        </div>
                    </div>

                    <!-- Conference Paper C1 -->
                    <div class="publication">
                        <span class="pub-label">C₁</span>
                        <div class="pub-content">
                            <div class="light-bold">BERT in Plutarch's Shadows.</div>
                            <div>Ivan P. Yamshchikov, Alexey Tikhonov, Yorgos Pantis, Charlotte Schubert, Jürgen Jost. <br>
                                <i>Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP 2022)</i>.
                            </div>
                            <div class="link-button" onclick="toggleAbstract('abstract-plutarch')">Abstract</div>
                            <div class="link-button" onclick="window.open('https://aclanthology.org/2022.emnlp-main.407/', '_blank')">Paper</div>
                            <div class="link-button" onclick="window.open('https://huggingface.co/altsoph/bert-base-ancientgreek-uncased', '_blank')">Code</div>
                            <div class="link-button" onclick="window.open('../files/bips_poster.pdf', '_blank')">Poster</div>
                            <div class="link-button" onclick="toggleBibtex('bibtex-plutarch')">BibTeX</div>

                            <div id="abstract-plutarch" class="abstract" style="display:none; border-left-color:#007bff;">
                                The extensive surviving corpus of the ancient scholar Plutarch of Chaeronea (ca. 45-120 CE) also contains several texts which, according to current scholarly opinion, did not originate with him and are therefore attributed to an anonymous author Pseudo-Plutarch. These include, in particular, the work Placita Philosophorum (Quotations and Opinions of the Ancient Philosophers), which is extremely important for the history of ancient philosophy. Little is known about the identity of that anonymous author and its relation to other authors from the same period. This paper presents a BERT language model for Ancient Greek. The model discovers previously unknown statistical properties relevant to these literary, philosophical, and historical problems and can shed new light on this authorship question. In particular, the Placita Philosophorum, together with one of the other Pseudo-Plutarch texts, shows similarities with the texts written by authors from an Alexandrian context (2nd/3rd century CE).
                            </div>
                            <div id="bibtex-plutarch" class="bibtex" style="display:none; border-left-color:#007bff;">
                                @inproceedings{yamshchikov2022bert,<br>
                                title={BERT in Plutarch’s Shadows},<br>
                                author={Yamshchikov, Ivan and Tikhonov, Alexey and Pantis, Yorgos and Schubert, Charlotte and Jost, J{\"u}rgen},<br>
                                booktitle={Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},<br>
                                pages={6071--6080},<br>
                                year={2022}<br>
                                }
                            </div>
                        </div>
                    </div>
                </section>
                <!-- Publications Section End -->
            </div>
        </main>

        <footer class="footer-text">
            <div class="container-fluid p-0 m-0">
                <div class="row no-gutters text-center d-flex justify-content-center m-0">
                    <div class="p-2">
                        <p class="text-center">© 2024 Yorgos Pantis — Last Update: Oct, 2025</p>
                    </div>
                </div>
            </div>
        </footer>
    </div>

    <script>
        function toggleAbstract(id) {
            const el = document.getElementById(id);
            el.style.display = (el.style.display === "none" || el.style.display === "") ? "block" : "none";
        }

        function toggleBibtex(id) {
            const el = document.getElementById(id);
            el.style.display = (el.style.display === "none" || el.style.display === "") ? "block" : "none";
        }
    </script>
</body>
</html>